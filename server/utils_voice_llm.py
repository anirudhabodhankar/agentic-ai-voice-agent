import threading  
import queue  
import re  
import os
import time
from typing import Dict, AsyncGenerator, Optional, Any
import asyncio  
import azure.cognitiveservices.speech as speechsdk
from opentelemetry import context as context_api
  
from langchain_core.runnables import Runnable  
from server import utils_logger
console_logger, console_tracer = utils_logger.get_logger_tracer()
  
class TextToGPTAudioStreamGenerator:  
    """  
    A class to handle the generation of audio streams from text using threading and asynchronous processing.  
      
    This class manages the flow of text generation, sentence segmentation, and audio chunk creation.  
    It utilizes multiple threads to handle different stages of the pipeline and provides an async  
    generator to yield audio chunks for playback.  
    """  
    
    @console_tracer.start_as_current_span("TextToGPTAudioStreamGenerator - init")
    def __init__(self) -> None:  
        """  
        Initializes the TextToGPTAudioStreamGenerator instance with necessary queues and threading events.  
        """  
        self.full_response: str = ""  
        # Queues for text chunks, sentences, and audio chunks  
        self.text_queue: queue.Queue = queue.Queue()  
        self.sentence_queue: queue.Queue = queue.Queue()  
        self.audio_queue: queue.Queue = queue.Queue()  
  
        # Events to signal completion of each stage  
        self.text_generation_complete: threading.Event = threading.Event()  
        self.sentence_generation_complete: threading.Event = threading.Event()  
        self.audio_generation_complete: threading.Event = threading.Event()  
        # self.audio_queue_iterator_complete: threading.Event = threading.Event()  

        #initialize the speech synthesizer
        pull_stream = speechsdk.audio.PullAudioOutputStream()
        stream_config = speechsdk.audio.AudioOutputConfig(stream=pull_stream)
        speech_config = speechsdk.SpeechConfig(subscription=os.getenv("AZURE_TTS_API_KEY", ""), region=os.getenv("AZURE_TTS_REGION", ""))
        speech_config.set_speech_synthesis_output_format(speechsdk.SpeechSynthesisOutputFormat.Raw8Khz16BitMonoPcm)
        speech_config.speech_synthesis_voice_name = os.getenv("AZURE_TTS_SYNTHESIS_VOICE_NAME", "hi-IN-AartiNeural")
        self.speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=stream_config)
        self.total_sentences = 0
        self.total_sentences_audio_complete = 0
        self.first_audio_chunk: bool = True
        self.total_audio_chunks = 0;
        self.total_audio_chunks_on_queue_iter = 0;
        self.total_audio_chunks_yield = 0;
        self.first_audio_chunk_span= None
        self.parent_context = None

    def az_speech_synthesis_callback(self, evt):
        """
        Callback function to handle speech synthesis events.
        """
        # Activate the parent context in this thread
        token = context_api.attach(self.parent_context) if self.parent_context else None

        if evt.result.reason == speechsdk.ResultReason.SynthesizingAudio:
            audio_chunk = evt.result.audio_data
            if self.first_audio_chunk:  
                self.first_audio_chunk = False  
                self.first_audio_chunk_span.end()
                console_logger.info(f'First audio chunk generated by tts of size: {len(audio_chunk)}')  
            
            self.audio_queue.put(audio_chunk)  
            self.total_audio_chunks += 1
        
        # Detach the context when done
        if token:
            context_api.detach(token)

    def az_speech_synthesis_completecallback(self, evt):
        # Activate the parent context in this thread
        token = context_api.attach(self.parent_context) if self.parent_context else None

        if evt.result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
            self.total_sentences_audio_complete += 1
            console_logger.info(f'total sentences : {self.total_sentences}, total_sentences_audio_complete : {self.total_sentences_audio_complete}')  
            
        # detach the context when done
        if token:
            context_api.detach(token)

    def get_full_response(self) -> str:  
        """  
        Retrieves the full accumulated text response.  
          
        Returns:  
            str: The full text response generated.  
        """  
        return self.full_response  
    
    async def generate_tokens(self, llm_agent_executor: Runnable, argument_dictionary: Dict[str, Any]  ) -> None:  
        """  
        Generate text chunks and place in text queue.  
        Runs in a separate thread.  
        """  
        # Activate the parent context in this thread
        token = context_api.attach(self.parent_context) if self.parent_context else None

        # Start text generation  
        first_text_chunk: bool = True 
        try:  
            async for event in llm_agent_executor.astream_events(argument_dictionary, version="v2"):  
                kind: str = event.get("event", "")  
                if kind == "on_chat_model_stream":  
                    new_text: str = event['data']['chunk'].content
                    if not new_text:  
                        continue  
  
                    if first_text_chunk:  
                        first_text_chunk = False  
                        console_logger.info(f'First text chunk generated of size: {len(new_text)}')  
  
                    self.full_response += new_text  
                    self.text_queue.put(new_text)  
  
        except Exception as e:  
            console_logger.error(f"Error in text generation: {e}")  
        finally:  
            self.text_generation_complete.set()  
            console_logger.info("Text token generation complete")  
        
        # Detach the context when done
        if token:
            context_api.detach(token)

    def generate_tokens_wrapper(self, llm_agent_executor: Runnable, argument_dictionary: Dict[str, Any]) -> None:  
        asyncio.run(self.generate_tokens(llm_agent_executor, argument_dictionary))

    def generate_sentences(self) -> None:  
        """  
        Processes text chunks from the text_queue, splits them into sentences,  
        and places the sentences into the sentence_queue.  
        Runs in a separate thread.  
        """  
        # Activate the parent context in this thread
        token = context_api.attach(self.parent_context) if self.parent_context else None

        sentence_buffer: str = ""  
        first_sentence_chunk: bool = True  
        sentence_pattern: re.Pattern = re.compile(r'[^.!ред?:\n\t]+[.!ред?:\n\t]')  
  
        while not (self.text_generation_complete.is_set() and self.text_queue.empty()):  
            try:  
                new_text: str = self.text_queue.get(timeout=0.1)  
                sentence_buffer += new_text  
                sentences: list = sentence_pattern.findall(sentence_buffer)  
  
                for sentence in sentences:  
                    stripped_sentence: str = sentence.strip()  
                    if(stripped_sentence == ""):
                        continue
                    if first_sentence_chunk:  
                        first_sentence_chunk = False  
                        console_logger.info(f'First sentence generated of size: {len(stripped_sentence)}')  
                    self.sentence_queue.put(stripped_sentence)  
                    self.total_sentences += 1
                    console_logger.info(f'Adding sentence to queue : [{stripped_sentence}], total sentences: {self.total_sentences}')
  
                # Retain any partial sentence in the buffer  
                sentence_buffer = re.sub(r'.*[.!ред?:\n\t]', '', sentence_buffer)  
            except queue.Empty:  
                continue  
  
        if sentence_buffer.strip():  
            self.sentence_queue.put(sentence_buffer.strip())  
  
        self.sentence_generation_complete.set()  
        console_logger.info("Text Sentence generation complete")  

        # Detach the context when done
        if token:
            context_api.detach(token)
 

    def generate_audio(self) -> None:  
        """  
        Converts sentences from the sentence_queue into audio chunks and places them into the audio_queue.  
        Runs in a separate thread.  
          
        Args:  
            llm_audio_generator (Runnable): An instance responsible for generating audio from text.  
        """  
        # Activate the parent context in this thread
        token = context_api.attach(self.parent_context) if self.parent_context else None
        self.speech_synthesizer.synthesizing.connect(self.az_speech_synthesis_callback)
        self.speech_synthesizer.synthesis_completed.connect(self.az_speech_synthesis_completecallback)

        while not (self.sentence_generation_complete.is_set() and self.sentence_queue.empty()):  
            try:  
                sentence: Optional[str] = self.sentence_queue.get(timeout=0.5)  
                if not sentence:  
                    continue  
  
                console_logger.info(f'Generating audio for sentence: {sentence}')  
  
                result = self.speech_synthesizer.speak_text_async(sentence).get()  
  
            except queue.Empty:  
                continue  
            except Exception as e:  
                console_logger.error(f"Error in generate_audio: {e}")  
                continue  

        while(self.total_sentences > self.total_sentences_audio_complete):
            console_logger.info(f'total sentences : {self.total_sentences}, total_sentences_audio_complete : {self.total_sentences_audio_complete}')            
            # sleep for 0.5 second to ensure all audio chunks are generated
            time.sleep(0.5)

        self.audio_generation_complete.set()
        console_logger.info("Audio chunk generation complete")

        # Detach the context when done
        if token:
            context_api.detach(token)
  
    async def audio_queue_iterator(self) -> AsyncGenerator[bytes, None]:  
        """  
        Asynchronously iterates over audio chunks in the audio_queue and yields them for playback.  
          
        Yields:  
            bytes: Audio chunk data.  
        """  
        # Activate the parent context in this thread
        token = context_api.attach(self.parent_context) if self.parent_context else None

        first_network_audio_chunk: bool = True  
  
        while not (self.audio_generation_complete.is_set() and self.audio_queue.empty()):  
            try:  
                audio_chunk: bytes = self.audio_queue.get(timeout=0.1)  
                if first_network_audio_chunk:  
                    first_network_audio_chunk = False  
                    console_logger.info(f'First audio chunk added to queue of size: {len(audio_chunk)}')  
                
                self.total_audio_chunks_on_queue_iter += 1
                yield audio_chunk  
            except queue.Empty:  
                await asyncio.sleep(0.01)  
                continue

        console_logger.info("Audio chunk Queue_iter complete")  
        # Detach the context when done
        if token:
            context_api.detach(token)
  
    # @console_tracer.start_as_current_span("generate_first_audio_chunk")
    async def generate_audio_chunks(  
        self,  
        llm_agent_executor: Runnable,  
        argument_dictionary: Dict[str, Any]  
    ) -> AsyncGenerator[bytes, None]:  
        """  
        Orchestrates the generation of audio chunks from text by managing threads and asynchronous operations.  
          
        Args:  
            llm_agent_executor (Runnable): Runnable instance for text generation.  
            argument_dictionary (Dict[str, str]): Dictionary of arguments for the agent executor.  
          
        Yields:  
            AsyncGenerator[bytes, None]: Yields audio chunks as they become available.  
        """  
        with console_tracer.start_as_current_span("generate_audio_chunks") as span:
            self.first_audio_chunk_span = console_tracer.start_span("first_audio_chunk")
            # Capture the current context to pass to threads
            self.parent_context = context_api.get_current()

            token_gen_thread: threading.Thread = threading.Thread(target=self.generate_tokens_wrapper, args=(llm_agent_executor,argument_dictionary,), daemon=True)
            token_gen_thread.start()  
    
            # Start sentence processing thread  
            sentence_gen_thread: threading.Thread = threading.Thread(target=self.generate_sentences, daemon=True)  
            sentence_gen_thread.start()  
    
            # Start audio generation thread  
            audio_gen_thread: threading.Thread = threading.Thread(target=self.generate_audio, args=(), daemon=True)
            audio_gen_thread.start()  
    
            # Yield audio chunks as they become available  
            async for audio_chunk in self.audio_queue_iterator():  
                self.total_audio_chunks_yield += 1
                yield audio_chunk  

            del self.speech_synthesizer
            # Ensure threads have completed  
            token_gen_thread.join()
            sentence_gen_thread.join()  
            audio_gen_thread.join()  

            console_logger.info(f'Total audio chunks generated: {self.total_audio_chunks}')
            console_logger.info(f'Total audio chunks on queue_iter: {self.total_audio_chunks_on_queue_iter}')
            console_logger.info(f'Total audio chunks yielded: {self.total_audio_chunks_yield}')
            console_logger.info("Audio chunk generation process complete")  